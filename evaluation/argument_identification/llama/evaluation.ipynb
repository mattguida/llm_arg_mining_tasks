{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation GM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6103448275862069,\n",
       " 'precision': 0.6103448275862069,\n",
       " 'recall': 0.23663101604278075,\n",
       " 'f1_score': 0.34104046242774566,\n",
       " 'total_predictions': 290,\n",
       " 'correct_predictions': 177,\n",
       " 'false_positives': 113,\n",
       " 'false_negatives': 571}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_predictions(golden_data_path, model_output_path):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions against a golden dataset.\n",
    "    - Correct predictions: if predicted argument is in golden arguments\n",
    "    - False positives: if predicted argument is not in golden arguments\n",
    "    - False negatives: if golden argument is not in predicted arguments\n",
    "    \"\"\"\n",
    "\n",
    "    golden_df = pd.read_csv(golden_data_path)\n",
    "    model_df = pd.read_csv(model_output_path)\n",
    "\n",
    "    # Store the golden arguments for each comment\n",
    "    golden_dict = {}\n",
    "    for _, row in golden_df.iterrows():\n",
    "        if row['label'] != 3:  # Exclude arguments not used\n",
    "            if row['comment_text'] not in golden_dict:\n",
    "                golden_dict[row['comment_text']] = set()\n",
    "            golden_dict[row['comment_text']].add(row['argument_text'])\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total_predictions = 0\n",
    "    correct_predictions = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    # Evaluate model predictions\n",
    "    for _, row in model_df.iterrows():\n",
    "        comment = row['comment_text']\n",
    "        predicted_argument = row['argument_text']\n",
    "\n",
    "        if comment in golden_dict:\n",
    "            total_predictions += 1\n",
    "            golden_arguments = golden_dict[comment]\n",
    "\n",
    "            # Check if the predicted argument is correct\n",
    "            if predicted_argument in golden_arguments:\n",
    "                y_true.append(predicted_argument)\n",
    "                y_pred.append(predicted_argument)\n",
    "                correct_predictions += 1\n",
    "            else:\n",
    "                # False positive: predicted argument does not match golden arguments\n",
    "                y_true.append(\"\")  # Append an empty string for no match\n",
    "                y_pred.append(predicted_argument)\n",
    "                false_positives += 1\n",
    "\n",
    "            # False negative: if there are other golden arguments that weren't predicted\n",
    "            for golden_arg in golden_arguments:\n",
    "                if golden_arg != predicted_argument:\n",
    "                    y_true.append(golden_arg)  # Append missed argument to y_true\n",
    "                    y_pred.append(\"\")  \n",
    "                    false_negatives += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    precision = correct_predictions / (correct_predictions + false_positives) if (correct_predictions + false_positives) > 0 else 0\n",
    "    recall = correct_predictions / (correct_predictions + false_negatives) if (correct_predictions + false_negatives) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"total_predictions\": total_predictions,\n",
    "        \"correct_predictions\": correct_predictions,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives\n",
    "    }\n",
    "\n",
    "golden_data_path = '/Users/guida/llm_argument_tasks/clean_data/GM_structured.csv'\n",
    "model_output_path = '/Users/guida/llm_argument_tasks/output_files/llama3/comarg_gm_argument_identification.csv'\n",
    "\n",
    "evaluate_predictions(golden_data_path, model_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation UGIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.4873417721518987,\n",
       " 'precision': 0.4873417721518987,\n",
       " 'recall': 0.21388888888888888,\n",
       " 'f1_score': 0.29729729729729726,\n",
       " 'total_predictions': 158,\n",
       " 'correct_predictions': 77,\n",
       " 'false_positives': 81,\n",
       " 'false_negatives': 283}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_predictions(golden_data_path, model_output_path):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions against a golden dataset.\n",
    "    - Correct predictions: if predicted argument is in golden arguments\n",
    "    - False positives: if predicted argument is not in golden arguments\n",
    "    - False negatives: if golden argument is not in predicted arguments\n",
    "    \"\"\"\n",
    "    \n",
    "    golden_df = pd.read_csv(golden_data_path)\n",
    "    model_df = pd.read_csv(model_output_path)\n",
    "\n",
    "    # Store the golden arguments for each comment\n",
    "    golden_dict = {}\n",
    "    for _, row in golden_df.iterrows():\n",
    "        if row['label'] != 3:  # Exclude arguments not used\n",
    "            if row['comment_text'] not in golden_dict:\n",
    "                golden_dict[row['comment_text']] = set()\n",
    "            golden_dict[row['comment_text']].add(row['argument_text'])\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total_predictions = 0\n",
    "    correct_predictions = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for _, row in model_df.iterrows():\n",
    "        comment = row['comment_text']\n",
    "        predicted_argument = row['argument_text']\n",
    "\n",
    "        if comment in golden_dict:\n",
    "            total_predictions += 1\n",
    "            golden_arguments = golden_dict[comment]\n",
    "\n",
    "            # Check if the predicted argument is correct\n",
    "            if predicted_argument in golden_arguments:\n",
    "                y_true.append(predicted_argument)\n",
    "                y_pred.append(predicted_argument)\n",
    "                correct_predictions += 1\n",
    "            else:\n",
    "                # False positive: predicted argument does not match golden arguments\n",
    "                y_true.append(\"\")  # Append an empty string for no match\n",
    "                y_pred.append(predicted_argument)\n",
    "                false_positives += 1\n",
    "\n",
    "            # False negative: if there are other golden arguments that weren't predicted\n",
    "            for golden_arg in golden_arguments:\n",
    "                if golden_arg != predicted_argument:\n",
    "                    y_true.append(golden_arg)  # Append missed argument to y_true\n",
    "                    y_pred.append(\"\")  \n",
    "                    false_negatives += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    precision = correct_predictions / (correct_predictions + false_positives) if (correct_predictions + false_positives) > 0 else 0\n",
    "    recall = correct_predictions / (correct_predictions + false_negatives) if (correct_predictions + false_negatives) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"total_predictions\": total_predictions,\n",
    "        \"correct_predictions\": correct_predictions,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives\n",
    "    }\n",
    "\n",
    "golden_data_path = '/Users/guida/llm_argument_tasks/clean_data/UGIP_structured.csv'\n",
    "model_output_path = '/Users/guida/llm_argument_tasks/output_files/llama3/comarg_ugip_argument_identification.csv'\n",
    "evaluate_predictions(golden_data_path, model_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
