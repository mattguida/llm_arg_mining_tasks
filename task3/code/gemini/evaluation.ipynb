{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from sklearn.metrics import jaccard_score\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(predictions_file: str, golden_data_file: str):\n",
    "\n",
    "    golden_data = pd.read_csv(golden_data_file)\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = [json.loads(line) for line in f]\n",
    "\n",
    "    predictions_dict = {pred['id']: pred['span'] for pred in predictions}\n",
    "\n",
    "    for index, row in golden_data.iterrows():\n",
    "        references.append([row['text'].split()])  \n",
    "\n",
    "        prediction_text = predictions_dict.get(row['id'])\n",
    "    \n",
    "        if prediction_text:  \n",
    "            candidates.append(prediction_text.split())\n",
    "        else:\n",
    "            print(f\"Warning: No prediction found for ID: {row['id']}\")\n",
    "            candidates.append([\"\"]) \n",
    "\n",
    "    print(f\"References count: {len(references)}, Candidates count: {len(candidates)}\")\n",
    "    \n",
    "\n",
    "    smoothing = SmoothingFunction().method4 \n",
    "    bleu_score = corpus_bleu(references, candidates, smoothing_function=smoothing)\n",
    "\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROGUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(predictions_file: str, golden_data_file: str):\n",
    "    golden_data = pd.read_csv(golden_data_file)\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = [json.loads(line) for line in f]\n",
    "\n",
    "    predictions_dict = {pred['id']: pred['span'] for pred in predictions}\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for index, row in golden_data.iterrows():\n",
    "        reference_text = row['text']\n",
    "        prediction_text = predictions_dict.get(row['id'], \"\")\n",
    "\n",
    "        score = scorer.score(reference_text, prediction_text)\n",
    "        for rouge_type in rouge_scores.keys():\n",
    "            rouge_scores[rouge_type].append(score[rouge_type].fmeasure)  \n",
    "\n",
    "    avg_rouge = {rouge_type: np.mean(scores) for rouge_type, scores in rouge_scores.items()}\n",
    "    \n",
    "    return avg_rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard(predictions_file: str, golden_data_file: str):\n",
    "    golden_data = pd.read_csv(golden_data_file)\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = [json.loads(line) for line in f]\n",
    "\n",
    "    predictions_dict = {pred['id']: pred['span'] for pred in predictions}\n",
    "\n",
    "    jaccard_scores = []\n",
    "\n",
    "    for index, row in golden_data.iterrows():\n",
    "        reference_tokens = set(row['text'].split())\n",
    "        prediction_tokens = set(predictions_dict.get(row['id'], \"\").split())\n",
    "        \n",
    "        if reference_tokens or prediction_tokens:\n",
    "            intersection = len(reference_tokens.intersection(prediction_tokens))\n",
    "            union = len(reference_tokens.union(prediction_tokens))\n",
    "            jaccard = intersection / union if union > 0 else 0\n",
    "            jaccard_scores.append(jaccard)\n",
    "        else:\n",
    "            jaccard_scores.append(0.0)\n",
    "    \n",
    "    avg_jaccard = np.mean(jaccard_scores)\n",
    "    \n",
    "    return avg_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No prediction found for ID: ab130\n",
      "Warning: No prediction found for ID: ab187\n",
      "Warning: No prediction found for ID: ab248\n",
      "Warning: No prediction found for ID: ab248\n",
      "Warning: No prediction found for ID: ab354\n",
      "Warning: No prediction found for ID: ab248\n",
      "Warning: No prediction found for ID: ab428\n",
      "Warning: No prediction found for ID: ab428\n",
      "References count: 739, Candidates count: 739\n",
      "BLEU Score: 0.0001477033110800935\n",
      "ROUGE Scores: {'rouge1': 0.30909457111762834, 'rouge2': 0.2955423031039821, 'rougeL': 0.3090034559199293}\n",
      "Jaccard Similarity: 0.2324933442351629\n"
     ]
    }
   ],
   "source": [
    "golden_data_file = '/Users/guida/llm_argument_tasks/clean_data/yru_abortion.csv' \n",
    "predictions_file = '/Users/guida/llm_argument_tasks/task3/output_files/gpt/yru_abortion_span_identification_gpt_original.jsonl'\n",
    " \n",
    "bleu_score = compute_bleu(predictions_file, golden_data_file)\n",
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "rouge_scores = compute_rouge(predictions_file, golden_data_file)\n",
    "print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "\n",
    "jaccard_score = compute_jaccard(predictions_file, golden_data_file)\n",
    "print(f\"Jaccard Similarity: {jaccard_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No prediction found for ID: ab130\n",
      "Warning: No prediction found for ID: ab187\n",
      "Warning: No prediction found for ID: ab248\n",
      "Warning: No prediction found for ID: ab248\n",
      "Warning: No prediction found for ID: ab354\n",
      "Warning: No prediction found for ID: ab248\n",
      "Warning: No prediction found for ID: ab428\n",
      "Warning: No prediction found for ID: ab428\n",
      "References count: 739, Candidates count: 739\n",
      "BLEU Score for abortion: 0.0001477033110800935\n",
      "ROUGE Scores for abortion: {'rouge1': 0.30909457111762834, 'rouge2': 0.2955423031039821, 'rougeL': 0.3090034559199293}\n",
      "Jaccard Similarity for abortion: 0.2324933442351629\n",
      "Warning: No prediction found for ID: gr62\n",
      "Warning: No prediction found for ID: gr81\n",
      "Warning: No prediction found for ID: gr99\n",
      "Warning: No prediction found for ID: gr109\n",
      "Warning: No prediction found for ID: gr208\n",
      "Warning: No prediction found for ID: gr208\n",
      "Warning: No prediction found for ID: gr234\n",
      "Warning: No prediction found for ID: gr420\n",
      "Warning: No prediction found for ID: gr432\n",
      "Warning: No prediction found for ID: gr473\n",
      "Warning: No prediction found for ID: gr481\n",
      "Warning: No prediction found for ID: gr510\n",
      "References count: 772, Candidates count: 772\n",
      "BLEU Score for gayRights: 0.0004599447062792282\n",
      "ROUGE Scores for gayRights: {'rouge1': 0.3135319008466973, 'rouge2': 0.29849186382801046, 'rougeL': 0.3133864666774022}\n",
      "Jaccard Similarity for gayRights: 0.23569612575864035\n",
      "Warning: No prediction found for ID: ma23\n",
      "Warning: No prediction found for ID: ma48\n",
      "Warning: No prediction found for ID: ma48\n",
      "Warning: No prediction found for ID: ma49\n",
      "Warning: No prediction found for ID: ma49\n",
      "Warning: No prediction found for ID: ma54\n",
      "Warning: No prediction found for ID: ma59\n",
      "Warning: No prediction found for ID: ma106\n",
      "Warning: No prediction found for ID: ma165\n",
      "Warning: No prediction found for ID: ma202\n",
      "Warning: No prediction found for ID: ma204\n",
      "Warning: No prediction found for ID: ma205\n",
      "Warning: No prediction found for ID: ma205\n",
      "Warning: No prediction found for ID: ma211\n",
      "Warning: No prediction found for ID: ma211\n",
      "Warning: No prediction found for ID: ma214\n",
      "Warning: No prediction found for ID: ma225\n",
      "Warning: No prediction found for ID: ma230\n",
      "Warning: No prediction found for ID: ma243\n",
      "Warning: No prediction found for ID: ma250\n",
      "Warning: No prediction found for ID: ma277\n",
      "Warning: No prediction found for ID: ma277\n",
      "Warning: No prediction found for ID: ma309\n",
      "Warning: No prediction found for ID: ma350\n",
      "Warning: No prediction found for ID: ma365\n",
      "Warning: No prediction found for ID: ma370\n",
      "Warning: No prediction found for ID: ma370\n",
      "Warning: No prediction found for ID: ma377\n",
      "Warning: No prediction found for ID: ma391\n",
      "Warning: No prediction found for ID: ma414\n",
      "Warning: No prediction found for ID: ma422\n",
      "References count: 691, Candidates count: 691\n",
      "BLEU Score for marijuana: 0.00029485366709349235\n",
      "ROUGE Scores for marijuana: {'rouge1': 0.27784580317539326, 'rouge2': 0.2621323446153552, 'rougeL': 0.2776722371205882}\n",
      "Jaccard Similarity for marijuana: 0.2013974445787924\n",
      "Warning: No prediction found for ID: oba5\n",
      "Warning: No prediction found for ID: oba14\n",
      "Warning: No prediction found for ID: oba104\n",
      "Warning: No prediction found for ID: oba126\n",
      "Warning: No prediction found for ID: oba155\n",
      "Warning: No prediction found for ID: oba178\n",
      "Warning: No prediction found for ID: oba197\n",
      "Warning: No prediction found for ID: oba220\n",
      "Warning: No prediction found for ID: oba223\n",
      "Warning: No prediction found for ID: oba232\n",
      "Warning: No prediction found for ID: oba238\n",
      "Warning: No prediction found for ID: oba274\n",
      "Warning: No prediction found for ID: oba278\n",
      "Warning: No prediction found for ID: oba341\n",
      "Warning: No prediction found for ID: oba353\n",
      "Warning: No prediction found for ID: oba374\n",
      "Warning: No prediction found for ID: oba383\n",
      "Warning: No prediction found for ID: oba383\n",
      "Warning: No prediction found for ID: oba398\n",
      "References count: 646, Candidates count: 646\n",
      "BLEU Score for obama: 4.7394274446102035e-05\n",
      "ROUGE Scores for obama: {'rouge1': 0.28162089362237586, 'rouge2': 0.2640984100337136, 'rougeL': 0.28162089362237586}\n",
      "Jaccard Similarity for obama: 0.21221648302532772\n"
     ]
    }
   ],
   "source": [
    "topics = ['abortion', 'gayRights', 'marijuana', 'obama']\n",
    "for topic in topics:\n",
    "\n",
    "    golden_data_file = f'/Users/guida/llm_argument_tasks/clean_data/yru_{topic}.csv'\n",
    "    predictions_file = f'/Users/guida/llm_argument_tasks/task3/output_files/gpt/yru_{topic}_span_identification_gpt_original.jsonl'\n",
    "\n",
    "    bleu_score = compute_bleu(predictions_file, golden_data_file)\n",
    "    print(f\"BLEU Score for {topic}: {bleu_score}\")\n",
    "\n",
    "    rouge_scores = compute_rouge(predictions_file, golden_data_file)\n",
    "    print(f\"ROUGE Scores for {topic}: {rouge_scores}\")\n",
    "\n",
    "    jaccard_score = compute_jaccard(predictions_file, golden_data_file)\n",
    "    print(f\"Jaccard Similarity for {topic}: {jaccard_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
