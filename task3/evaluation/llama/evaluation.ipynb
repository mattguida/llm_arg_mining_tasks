{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from sklearn.metrics import jaccard_score\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bleu(predictions_file: str, golden_data_file: str):\n",
    "\n",
    "    golden_data = pd.read_csv(golden_data_file)\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = [json.loads(line) for line in f]\n",
    "\n",
    "    predictions_dict = {pred['id']: pred['span'] for pred in predictions}\n",
    "\n",
    "    for index, row in golden_data.iterrows():\n",
    "        references.append([row['text'].split()])  \n",
    "\n",
    "        prediction_text = predictions_dict.get(row['id'])\n",
    "    \n",
    "        if prediction_text:  \n",
    "            candidates.append(prediction_text.split())\n",
    "        else:\n",
    "            print(f\"Warning: No prediction found for ID: {row['id']}\")\n",
    "            candidates.append([\"\"]) \n",
    "\n",
    "    print(f\"References count: {len(references)}, Candidates count: {len(candidates)}\")\n",
    "    \n",
    "\n",
    "    smoothing = SmoothingFunction().method4 \n",
    "    bleu_score = corpus_bleu(references, candidates, smoothing_function=smoothing)\n",
    "\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROGUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(predictions_file: str, golden_data_file: str):\n",
    "    golden_data = pd.read_csv(golden_data_file)\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = [json.loads(line) for line in f]\n",
    "\n",
    "    predictions_dict = {pred['id']: pred['span'] for pred in predictions}\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for index, row in golden_data.iterrows():\n",
    "        reference_text = row['text']\n",
    "        prediction_text = predictions_dict.get(row['id'], \"\")\n",
    "\n",
    "        score = scorer.score(reference_text, prediction_text)\n",
    "        for rouge_type in rouge_scores.keys():\n",
    "            rouge_scores[rouge_type].append(score[rouge_type].fmeasure)  \n",
    "\n",
    "    avg_rouge = {rouge_type: np.mean(scores) for rouge_type, scores in rouge_scores.items()}\n",
    "    \n",
    "    return avg_rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard(predictions_file: str, golden_data_file: str):\n",
    "    golden_data = pd.read_csv(golden_data_file)\n",
    "    references = []\n",
    "    candidates = []\n",
    "\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = [json.loads(line) for line in f]\n",
    "\n",
    "    predictions_dict = {pred['id']: pred['span'] for pred in predictions}\n",
    "\n",
    "    jaccard_scores = []\n",
    "\n",
    "    for index, row in golden_data.iterrows():\n",
    "        reference_tokens = set(row['text'].split())\n",
    "        prediction_tokens = set(predictions_dict.get(row['id'], \"\").split())\n",
    "        \n",
    "        if reference_tokens or prediction_tokens:\n",
    "            intersection = len(reference_tokens.intersection(prediction_tokens))\n",
    "            union = len(reference_tokens.union(prediction_tokens))\n",
    "            jaccard = intersection / union if union > 0 else 0\n",
    "            jaccard_scores.append(jaccard)\n",
    "        else:\n",
    "            jaccard_scores.append(0.0)\n",
    "    \n",
    "    avg_jaccard = np.mean(jaccard_scores)\n",
    "    \n",
    "    return avg_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No prediction found for ID: ab130\n",
      "Warning: No prediction found for ID: ab187\n",
      "Warning: No prediction found for ID: ab248\n",
      "Warning: No prediction found for ID: ab248\n",
      "Warning: No prediction found for ID: ab354\n",
      "Warning: No prediction found for ID: ab248\n",
      "Warning: No prediction found for ID: ab428\n",
      "Warning: No prediction found for ID: ab428\n",
      "References count: 739, Candidates count: 739\n",
      "BLEU Score: 0.0001477033110800935\n",
      "ROUGE Scores: {'rouge1': 0.30909457111762834, 'rouge2': 0.2955423031039821, 'rougeL': 0.3090034559199293}\n",
      "Jaccard Similarity: 0.2324933442351629\n"
     ]
    }
   ],
   "source": [
    "golden_data_file = '/Users/guida/llm_argument_tasks/clean_data/yru_abortion.csv' \n",
    "predictions_file = '/Users/guida/llm_argument_tasks/task3/output_files/gpt/yru_abortion_span_identification_gpt_original.jsonl'\n",
    " \n",
    "bleu_score = compute_bleu(predictions_file, golden_data_file)\n",
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "rouge_scores = compute_rouge(predictions_file, golden_data_file)\n",
    "print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "\n",
    "jaccard_score = compute_jaccard(predictions_file, golden_data_file)\n",
    "print(f\"Jaccard Similarity: {jaccard_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No prediction found for ID: ab122\n",
      "Warning: No prediction found for ID: ab140\n",
      "Warning: No prediction found for ID: ab362\n",
      "Warning: No prediction found for ID: ab362\n",
      "References count: 739, Candidates count: 739\n",
      "BLEU Score for abortion: 0.0031015684827588796\n",
      "ROUGE Scores for abortion: {'rouge1': 0.3655652296319185, 'rouge2': 0.34939350301098265, 'rougeL': 0.36396002794931065}\n",
      "Jaccard Similarity for abortion: 0.2868758393900423\n",
      "Warning: No prediction found for ID: gr81\n",
      "Warning: No prediction found for ID: gr99\n",
      "Warning: No prediction found for ID: gr109\n",
      "Warning: No prediction found for ID: gr117\n",
      "Warning: No prediction found for ID: gr208\n",
      "Warning: No prediction found for ID: gr208\n",
      "References count: 772, Candidates count: 772\n",
      "BLEU Score for gayRights: 0.0027138748576297973\n",
      "ROUGE Scores for gayRights: {'rouge1': 0.3354544808813793, 'rouge2': 0.31881002677809006, 'rougeL': 0.3337367040772927}\n",
      "Jaccard Similarity for gayRights: 0.25457605128378646\n",
      "Warning: No prediction found for ID: ma59\n",
      "Warning: No prediction found for ID: ma140\n",
      "Warning: No prediction found for ID: ma165\n",
      "Warning: No prediction found for ID: ma180\n",
      "Warning: No prediction found for ID: ma202\n",
      "Warning: No prediction found for ID: ma213\n",
      "Warning: No prediction found for ID: ma225\n",
      "Warning: No prediction found for ID: ma252\n",
      "Warning: No prediction found for ID: ma274\n",
      "Warning: No prediction found for ID: ma282\n",
      "Warning: No prediction found for ID: ma297\n",
      "Warning: No prediction found for ID: ma329\n",
      "Warning: No prediction found for ID: ma342\n",
      "Warning: No prediction found for ID: ma377\n",
      "Warning: No prediction found for ID: ma414\n",
      "References count: 691, Candidates count: 691\n",
      "BLEU Score for marijuana: 0.0035679707851871607\n",
      "ROUGE Scores for marijuana: {'rouge1': 0.3297349200282866, 'rouge2': 0.3147923114685774, 'rougeL': 0.3295915320645393}\n",
      "Jaccard Similarity for marijuana: 0.24737775305585805\n",
      "Warning: No prediction found for ID: oba171\n",
      "Warning: No prediction found for ID: oba184\n",
      "Warning: No prediction found for ID: oba221\n",
      "Warning: No prediction found for ID: oba274\n",
      "References count: 646, Candidates count: 646\n",
      "BLEU Score for obama: 0.0012717868823816146\n",
      "ROUGE Scores for obama: {'rouge1': 0.34570643427196207, 'rouge2': 0.32993342008629234, 'rougeL': 0.34563679058819974}\n",
      "Jaccard Similarity for obama: 0.26776112577685374\n"
     ]
    }
   ],
   "source": [
    "topics = ['abortion', 'gayRights', 'marijuana', 'obama']\n",
    "for topic in topics:\n",
    "\n",
    "    golden_data_file = f'/Users/guida/llm_argument_tasks/clean_data/yru_{topic}.csv'\n",
    "    predictions_file = f'/Users/guida/llm_argument_tasks/task3/output_files/llama/yru_{topic}_span_identification_llama_original.jsonl'\n",
    "\n",
    "    bleu_score = compute_bleu(predictions_file, golden_data_file)\n",
    "    print(f\"BLEU Score for {topic}: {bleu_score}\")\n",
    "\n",
    "    rouge_scores = compute_rouge(predictions_file, golden_data_file)\n",
    "    print(f\"ROUGE Scores for {topic}: {rouge_scores}\")\n",
    "\n",
    "    jaccard_score = compute_jaccard(predictions_file, golden_data_file)\n",
    "    print(f\"Jaccard Similarity for {topic}: {jaccard_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
